### Word Embeddings
#### Question One: Why is using one-hot encoding an inefficient towards vectorizing a corpus of words?  How are word embeddings different? 

One-hot encoding is inefficient when vetorizing a corpus of words because a one-hot encoding vector is sparse; so if we were to create a one-hot encoding vector with many sentences we would be creating a zero vector that was as long as the vocabulary with a one at the index that corresponds to each word. This would mean that a majority of the elements in the vector are zero and would end up taking much more time than necessary to vectorize a group of words. A more efficient way to vectorize words is with word embedding which works with/represents analogous reasoning which shows how words/ word pairings relate to each other based on the vector difference between paris of words. Often word embeddings use cosine similarity to define the similiarity between two vectors - cosine similiarity works with an equation such that the inner product of the two vectors (if they are similar this value will be large) is divided by the euclidean distance between the two vectors (which calculates the cosine angle between the two vectors). Due to the fact that word embeddings operate on analogous reasoning, word embeddings are favorable because of the generality of these analygous relations between words. 

#### Question Two: Compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Post your plots and describe them.
![WELoss](https://user-images.githubusercontent.com/67922294/88744978-dcac8180-d116-11ea-9958-2359b4bb91c0.png)

![WEAccuracy](https://user-images.githubusercontent.com/67922294/88745029-fbab1380-d116-11ea-8f4c-07cda2b7d82b.png)

#### Question Three (Stretch Goal):  Follow the link to the Embedding Projector provided at the end of the exercise.  Produce the visualization of your embeddings.  Interpret your visualization.  What is it describing?  Is there relevance with regard to words that are proximate to each other?

### Text Classification with an RNN

#### Question One: Again compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Stack two or more LSTM layers in your model.  Post your plots and describe them
