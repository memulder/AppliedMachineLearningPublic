### Problem Statement 
A food desert is a geographic area where residents have limited access (or no access) to healthy affordable food. These deserts are most commonly found in urban, low-income and majority minority communities. Those that reside in food deserts suffer from statistically higher rates of obesity, type 2 diabetes, cardiovascular disease, and other diet-related conditions than the general population. Food deserts are a little-known public health crisis that need to be better examined and combatted. This link between health and food deserts allows for an interesting opportunity to employ a machine learning algorithm. Using a machine learning algorithm would it be possible to predict where food deserts are based solely on the health data of a community (i.e using only health data, like obesity or diabetes rates, is it possible to accurately predict/identify where food deserts are in America?) Food, healthy food specifically, is a necessity for a long and healthful life. It is not just enough to provide people with food, it must be high-quality food, because in the long run even when people have access to food (but only food that is unhealthy) the health complications that arise from unhealthy eating only serve to cut peoples lives short. If this model is accurate, it will serve to highlight the importance of access to healthy food to the health adn survival of a community.

### Describe your implementation of the cats & dogs exercise. How did you setup the data?
Once I read in the cats and dogs images the cat images were placed in one folder and the dog images in another. The images in the cat folder were them split into a training directory and a testing directory amd the dog images were split in the same manner. Later the images were split into a training directory and a smaller set of images was sorted into a validation directory meant to make test the accuracy of the model. 

####  Question One: Which optimizer have you selected, and how might it compare to other possible choices?  
For this lab I used the RMSprop optimizer. RMSprop is unique as an optimizer because it tries to find a single global learning rate for the entire algorithm by considering both the size of the step for each weight as well as the sign of the gradient at any given point. If, after taking a step, the sign of the magnitude of the gradient are the same, then the optimizer knows it is moving in the right direction, if the signs are different then it is a step in the wrong direction. As the step size adapts over time the rate of learning accelerate and it does so in the correct direction. RMSprop also keeps the moving average of the squared gradients for each weight so that the mini-batches are divided by a similar number each time. This setup allows RMSprop to be a quick and incredibly useful optimizer. 

#### Question two: Describe your selected loss function and it’s implementation. How is it effectively penalizing bad predictions? 
I selected binary crossentropy as my loss function for this code. Since this is a binary classification problem, after fitting the model, the model assigns each data point with a probability of it belonging to one of the classes. After the model produces the predicted probabilities, the loss function then evaluates how accurate these predictions were based on the true values of each data point. A high probability score indicates low loss, whereas a low probability score indicated high loss. The overall loss is caluclated by finding the negative log of each probability and then computing the mean of all the losses. Specifically, with cross-entropy, this loss function computes the entropy (or the uncertainty in regards to a data point belonging to a certain class) of the true values of the distribution of data points as well as of the predicted values for the distribution. The closer the two values are the lower the cross-entropy. A low cross-entropy value is desirable as it indicates that the predicted value was close to the true value of the data point.  

#### Question 3: What is the purpose of the metric= argument in your model.compile() function?
The metrics argument in my model.compile function is meant to judge the performance of the model (but the results arent used for training the model). In this case the metric that was being tracked in this model was the accuracy of the model, or how often the predicted labels matched the actual labels of the data given to the model. 

#### Question 4: Plot the accuracy and loss results for both the training and test datasets.  Include these in your response.  Assess the model and describe how good you think it performed.
![validationaccuracy](https://user-images.githubusercontent.com/67922294/87975622-dd4f7300-ca99-11ea-8d50-a6bdc8c80f90.png)
![validationloss](https://user-images.githubusercontent.com/67922294/87975760-0cfe7b00-ca9a-11ea-984f-18c4a551b8a9.png)

These graphs show that the model is relatively accurate, the validation accuracy score was around 80% and the traning accuracy score was above 95%. However, the training accuracy is higher than the validation accuracy which indicates that the model is overfit. Additionall, the validation loss curve is going up which is another indication that the model is overfit.

#### Question five: Use the model to predict 3 dog images and 3 cat images.  Upload you images and the prediction.  How did your model perform in practice?  Do you have any ideas of how to improve the model’s performance?
These were the images I chose to testg. Three of them are of my cat Poncho (the orange one), one was of my kittens,Louie and Adele, and two were of my dog, Abby.

![catsdogs](https://user-images.githubusercontent.com/67922294/87971666-aece9980-ca93-11ea-960a-12e7dec3ebc1.png)

Truth be told, this model performed horribly in practice. Of the four cat pictures it guessed that ALL of them were dogs and of the dog pictures it only accurately predicted that one of them was a dog. The only picutre this model accurately predicted was the picture of my dog lying belly-up in her dog bed, which I thought the model would struggle with the most. I think perhaps the lighting and position of the animals had some impact on how well the model worked, but overall it did not do well. Perhaps to improve this model I could try to use different optimizer and loss functions or increase the amount of epochs the training data has to go through. Additionally better feature selection might be useful, but it would be hard to decide on which features to focus on since cats and dogs look so much alike. 
