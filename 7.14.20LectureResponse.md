### Last week you did an exercise where you manually applied a 3x3 array as a filter to an image of two people ascending an outdoor staircase.  Modify the existing filter and if needed the associated weight in order to apply your new filters to the image 3 times.  Plot each result, upload them to your response, and describe how each filter transformed the existing image as it convolved through the original array and reduced the object size.  What are you functionally accomplishing as you apply the filter to your original array (see the following snippet for reference)?  Why is the application of a convolving filter to an image useful for computer vision?  Stretch goal: instead of using the misc.ascent() image from scipy, can you apply three filters and weights to your own selected image?  Again describe the results.

![filter1](https://user-images.githubusercontent.com/67922294/87457695-cd85e980-c5d6-11ea-8442-66784806ddf7.png)
1. filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]], weight = 1

This image very faintly pulled out the vertical lines along the left-hand side of the image. 


![filter2](https://user-images.githubusercontent.com/67922294/87458122-7b919380-c5d7-11ea-8e2e-ba603fb4ee97.png)
2. filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], weight = 1

This image pulled out the vertical lines in teh image and make them really stand out. 

![filter3](https://user-images.githubusercontent.com/67922294/87458244-b3004000-c5d7-11ea-931f-e01ee8bd28ec.png)
3. fliter = [ [1, 1, 1], [1, 2, 1], [1, 1, 1]], weight = .1

This image essentially just returned the original image to me. I beleive this is because the filter was so evenly spread out adn the changes were well-balanced and minimal at best, so very little was actually being changed. 

A convolution is a filter that passes over an image, processing it, and extracting features that show a commonolatity in the image. This ability to extract commonalities in images is what makes a convolving filter image so useful for computer vision; by allowing the computer to pick up on commonalities, the computer has more flexibility/ lee-way in terms of identifying images. With convulving filters, the computer no longer is just looking at the raw pixels of an image but is now able to identify the actual features thatt make up an object, so there is more flexibility in terms of correctly identifying objects not matter what form/perspective they show up in. To Generate a convolutions you scan every pixel in the image and then look at it's neighboring pixels. Then you multiply out the values of these pixels by the equivalent weights in a filter.

### B.Another useful method is pooling.  Apply a 2x2 filter to one of your convolved images, and plot the result.  In effect what have you accomplished by applying this filter?  Can you determine from the code which type of pooling filter is applied, and the method for selecting a pixel value (see the following snippet)?  Did the result increase in size or decrease?  Why would this method be method?  Stretch goal:  again, instead of using misc.ascent(), apply the pooling filter to one of your transformed images.

### C.The lecture for today (Coding with Convolutional Neural Network) compared the application of our previously specified deep neural network with a newly specified convolutional neural network.  Instead of using the fashion_MNIST dataset, use the mnist dataset (the hand written letters) to train and compare your DNN and CNN output.      Were you able to improve your model by adding the Conv2D and MaxPooling2D layers to your neural network?  Plot the convolutions graphically, include them in your response and describe them.  Edit the convolutions be changing the 32s to either 16 or 64 and describe what impact this had on accuracy and training time.  What happens if you add more convolution layers?
