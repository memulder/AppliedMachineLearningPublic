#### Convolutional horses and humans

##### Question 1 : Describe the ImageDataGenerator() command and its associated argument.  What objects and arguments do you need to specify in order to flow from the directory to the generated object?  What is the significance of specifying the target_size = as it relates to your source images of varying sizes? What considerations might you reference when programming the class mode = argument?  What difference exists when applying the ImageDataGenerator() and .flow_from_directory() commands to the training and test datasets?
The ImageDataGenerator command essentially creates generators of batches of data/images that have been transformed in some way. In our case, used the argument "reshape" to reshape the pixel values so that the values were between 0 and 1 and no longer from 0 to 255. There are many additional arguments that you can supply as well, but they mostly deal with telling the generator how you want to transform the image. The command can also read images from the subdirectories provided to it and label the images with the appropriate labels from the appropriate subdirectory. To flow from the directory to the generated object you must use ImageDataGenerator.flow_from_directory() and specify which directory you want to get the images from. You can also specify the labels you want the images to be given, the batch size, the target size, and the class mode, which in our case was binary. As opposed to the arguments for ImageDataGenerator, the arguments for .flow_from_directory() are more about telling the computer how to classify the images provided. By setting your target size you can transform all the images so that they are outputted (sorry if this is not a real word!) in the same size and thereby easier to use, interpret, and compare. When programming the class mode it is a good idea to consider what type of/how many labels you want the generator to return in regards to your image. For instance if you have multiple possible classifications you might use class_mode = 'categorical' or if you only have two possible labels you might use class_mode = 'binary. 


##### Question 2 : Describe the model architecture of the horses and humans CNN as you have specified it.  Did you modify the number of filters in your Conv2D layers?  How do image sizes decrease as they are passed from each of your Conv2D layers to your MaxPooling2D layer and on to the next iteration?  Finally, which activation function have you selected for your output layer?  What is the significance of this argument’s function within the context of your CNN’s prediction of whether an image is a horse or a human?  What functions have you used in the arguments of your model compiler?
I created three convolution and pooling layers, one flatten layer, and two dense layers, finishing off with a sigmoid activation as this is a binary classification example so the only classification options are 0 or 1 (horse or human). The convolution layers always dealt with 3x3 filters whereas the pooling layers always dealt with 2x2 layers. The image sizes decrease by about half as they were passed from each of my Conv2D layers to my MaxPooling2D layer and on to the next iteration. In my model compiler I specified the function (binary_crossentropy, since this is a binary classification problem), the optimizer function (RMSprop, which automates learning-rate tuning), and the metrics (accuracy) I want the model to keep track of. 

#### Regression

##### Question 1 : Using the auto-mpg dataset (auto-mpg.data), upload the image where you used the seaborn library to pairwise plot the four variables specified in your model.  Describe how you could use this plot to investigate the co-relationship amongst each of your variables.  Are you able to identify interactions amongst variables with this plot?  What does the diagonal access represent?  Explain what this function is describing with regarding to each of the variables.

![regressionpairplot](https://user-images.githubusercontent.com/67922294/87688133-989fa100-c754-11ea-9427-f90b0094997b.png)

This function is showing how each of the variables interact with each other; it looks at the relationship between one variable and another variable and compares each variable with another variable in all possible comparisons which allows you to easily see if there are any relationships between variables in your dataset to be investigated further. For instance, in this plot it seems like there is some exponential relationships amongst some of the variables. Along the diagonal access you can see how the variable relates to itself. 

##### Question 2 : After running model.fit() on the auto-mpg.data data object, you returned the hist.tail() from the dataset where the training loss, MAE & MSE were recorded as well as those same variables for the validating dataset.  What interpretation can you offer when considering these last 5 observations from the model output?  Does the model continue to improve even during each of these last 5 steps?  Can you include a plot to illustrate your answer?  Stretch goal: include and describe the final plot that illustrates the trend of true values to predicted values as overlayed upon the histogram of prediction error.  
![regressiontail](https://user-images.githubusercontent.com/67922294/87689123-d650f980-c755-11ea-89fe-af23c8ca1bdf.png)

![MAEofMPG](https://user-images.githubusercontent.com/67922294/87689342-16b07780-c756-11ea-8b15-13eb82341c88.png)

![MSEofMPG](https://user-images.githubusercontent.com/67922294/87689674-7e66c280-c756-11ea-8a0d-5cc2f9f1aad0.png)

![earlystopregression](https://user-images.githubusercontent.com/67922294/87690141-0e0c7100-c757-11ea-9b12-26c7ab06ca16.png)

As seen in the above table and graphs during the last five epochs the models does not appear to be improving by any great measure. To illitrate this point, as you can see the MSE and MAE scores seem to level off towards the end of the graphs as the number of epochs increases. 

![predictionsplot](https://user-images.githubusercontent.com/67922294/87690387-65aadc80-c757-11ea-83c8-260815e0979e.png)

![errordistribution](https://user-images.githubusercontent.com/67922294/87690520-8ffc9a00-c757-11ea-86be-4bb09ef0760b.png)

As you can see by these plots the model predicts the true values relatively well. There is a normal distibution of error around 2.

#### Overfit and underfit
##### Question 1 : What was the significance of comparing the 4 different sized models (tiny, small, medium, large)?  Can you include a plot to illustrate your answer?

By compoaring the 4 different sized models we were able to see how the size of a model affects the loss and validation scores of each model. In this case, we were able to see that smaller models had lower scores in terms of validation loss, but the larger models were overfit as seen in the fact that their training and validation loss scores diverged so drastically, with the validation loss score increasing rapidly as the trainng score increased as well. 

