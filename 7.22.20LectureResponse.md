### Boosted Trees
#### Question one: What is a one-hot-encoded column and why might it be needed when transforming a feature?  Are the source values continuous or discrete?

A one-hot-encoded column is a column that holds values that have been transformed from a categorical value to a numberic value, which is why it is useful: a one-hot-encoded columm allows us to represent categorical values as numeric values so that our algorithms can actually deal with the information expressed in the categorical values. It houses discrete values so that values simply signal when teh feature of the column is true or not true for an observation in the data. 

#### Question two: What is a dense feature?  For example, if you execute example = dict(dftrain) and then tf.keras.layers.DenseFeatures(your_features)(your_object).numpy(), how has the content of your data frame been transformed?  Why might this be useful?

A dense feature is an array that replaces all of the places in the data without values with a zero. As such, it can specify the absense of some values and can help locate a value in the data. if I were to execute example = dict(dftrain) and then tf.keras.layers.DenseFeatures(your_features)(your_object).numpy(), my data would come in the form of an array where all of the values would be the same as before, but in the places between values where there was no value before, there are now zeros. Because a dense feature can locate all of the areas where nothing occurs in the data it is helpful when trying to identify boundaries. 

#### Question three: Provide a histogram of the probabilities for the logistic regression as well as your boosted tree model.  How do you interpret the two different models?  Are their predictions essentially the same or is there some area where they are noticeable different.  Plot the probability density function of the resulting probability predictions from the two models and use them to further illustrate your argument.  Include the ROC plot and interpret it with regard to the proportion of true to false positive rates, as well as the area under the ROC curve.  How does the measure of the AUC reflect upon the predictive power of your model?

![histPredictedProbs](https://user-images.githubusercontent.com/67922294/88400756-c7da9180-cd96-11ea-9bbd-f769d3646d39.png)

Red = logistic regression
Green = boosted trees

![DistPlot](https://user-images.githubusercontent.com/67922294/88401880-4f74d000-cd98-11ea-9638-97820109e853.png)

Blue = Boosted Trees
Orange = Logistic Regression

![ROC](https://user-images.githubusercontent.com/67922294/88402135-b09ca380-cd98-11ea-9d7d-f35cf73d93c9.png)

Based on the histogram you can see that the boosted trees was more far-reaching in the probabilities it was predicting: logistic regression seemed to be a bit less sure of its predictions as its historgram tended to be more centered in terms of the probability distribution whereas the  boosted tree had a high frequency of predicting death (the spike/tall bin around .2 on the left-hand side) and of predicting survival (the spike/tall bin around 1 on the right-hand side), though it definitely predicted death more often than not. You also see this phenomenon in the probability density function. Both models often predicted death as seen by the tall curves around .2, but the boosted trees model has a second increasing curve around 1, whereas the logistic regression model had a higher frequency of predicting values between .5 and .7 and a very low frequency of predicting any values around one. 

In looking at the ROC plot you can see that the model is relatively accurate. The curve gets close to .8 or 80% accuracy in terms of correctly predicting the outcome. With an ROC plot the area under the curve is equal to the probability that a model will rank a randomly chosen positive instance higher than a randomly chosen negative one. There is a great deal of area under the curve from 0 to .2 along the x-axis (false positive rate) which indicates that there were many observations that the model correctly predicted
### Boosted Trees (with model understanding)
#### Question one: Upload your feature values contribution to predicted probability horizontal bar plot as well as your violin plot.  Interpret and discuss the two plots.  Which features appear to contribute the most to the predicted probability?

![FeaturesContributionPlot](https://user-images.githubusercontent.com/67922294/88403679-d7f47000-cd9a-11ea-9d42-86133f08d2a7.png)

![ViolinPlot](https://user-images.githubusercontent.com/67922294/88403880-1db13880-cd9b-11ea-8071-ef108689f63b.png)

#### Question two: Upload at least 2 feature importance plots.  Which features are the most important in their contribution to your models predictive power?

![GainFeatureImportance](https://user-images.githubusercontent.com/67922294/88405018-abd9ee80-cd9c-11ea-9054-12aea8b4853c.png)
![MeanDFC](https://user-images.githubusercontent.com/67922294/88405331-1ab74780-cd9d-11ea-896a-93e9b42be620.png)

It seems to be that ones sex, fare, class (which is tied to fare;the higher the fare the higher the likelihood that that person is of a higher class) and age are the most important to my models predictive power. 
